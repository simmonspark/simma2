# hey use 2b
#at config -> get_config_for_2b()
'''
vocab_size=256000
max_position_embeddings=8192
num_hidden_layers=18
num_attention_heads=8
num_key_value_heads=1
hidden_size=2048
intermediate_size=16384
head_dim=256
rms_norm_eps=1e-06
dtype='bfloat16'
quant=False
tokenizer='tokenizer/tokenizer.model'
attn_types=None
sliding_window_size=None
final_logit_softcapping=None
attn_logit_softcapping=None
query_pre_attn_scalar=None
use_pre_ffw_norm=False
use_post_ffw_norm=False
'''

